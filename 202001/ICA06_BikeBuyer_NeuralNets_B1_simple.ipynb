{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Declare needed Python packages (done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#packages for manipulating data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#packages for visualizing data\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "# import graphviz\n",
    "# import pydotplus\n",
    "import io\n",
    "from scipy import misc\n",
    "\n",
    "\n",
    "#packages for preprocessing data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "#packages for data mining\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#package for evaluate datamining model in term of accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "\n",
    "#package for connecting to SQL Server\n",
    "import pyodbc\n",
    "\n",
    "#setting up environment\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns',500 )\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   #turn off warning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Read csv data to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>MaritalStatus_Num</th>\n",
       "      <th>Gender_Num</th>\n",
       "      <th>YearlyIncome</th>\n",
       "      <th>TotalChildren</th>\n",
       "      <th>NumberChildrenAtHome</th>\n",
       "      <th>EnglishEducation_Num</th>\n",
       "      <th>EnglishOccupation_Num</th>\n",
       "      <th>HouseOwnerFlag</th>\n",
       "      <th>NumberCarsOwned</th>\n",
       "      <th>Age</th>\n",
       "      <th>BikeBuyer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>10000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  MaritalStatus_Num  Gender_Num  YearlyIncome  TotalChildren  \\\n",
       "0           0                  1           1       20000.0              0   \n",
       "1           1                  0           1       20000.0              0   \n",
       "2           2                  1           1       10000.0              0   \n",
       "3           3                  1           0       10000.0              1   \n",
       "4           4                  1           0       10000.0              1   \n",
       "\n",
       "   NumberChildrenAtHome  EnglishEducation_Num  EnglishOccupation_Num  \\\n",
       "0                     0                     3                      2   \n",
       "1                     0                     3                      2   \n",
       "2                     0                     1                      0   \n",
       "3                     1                     1                      0   \n",
       "4                     1                     1                      0   \n",
       "\n",
       "   HouseOwnerFlag  NumberCarsOwned  Age  BikeBuyer  \n",
       "0               0                0   36          1  \n",
       "1               1                0   37          1  \n",
       "2               0                2   39          1  \n",
       "3               0                0   41          1  \n",
       "4               0                0   38          1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(r'C:\\users\\vdavis\\numeric_vTargetMail_ICA04a.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Training and testing\n",
    "How do you know how well a model will generalize to new cases? A common option is to split your data into two sets: the training set and the test set. As these names imply, you train your model using the training set, and you test it using the test set. The error rate on new cases is called the generalization error (or out-of-sample error), and by evaluating your model on the test set, you get an estimation of this error. This value tells you how well your model will perform on instances it has never seen before.\n",
    "\n",
    "The SciKit library provides a tool, called the Model Selection library. There’s a class in the library which is, aptly, named ‘train_test_split.’\n",
    "\n",
    "test_size — This parameter decides the size of the data that has to be split as the test dataset. This is given as a fraction. For example, if you pass 0.5 as the value, the dataset will be split 50% as the test dataset. If you’re specifying this parameter, you can ignore the next parameter.\n",
    "train_size — You have to specify this parameter only if you’re not specifying the test_size. This is the same as test_size, but instead you tell the class what percent of the dataset you want to split as the training set.\n",
    "random_state — Here you pass an integer, which will act as the seed for the random number generator during the split. Or, you can also pass an instance of the RandomState class, which will become the number generator. If you don’t pass anything, the RandomState instance used by np.random will be used instead.\n",
    "We split our dataset (data) into two different datasets training and test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Split data\n",
    "We split our dataset (data) into two different datasets: training and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training size: 17484; Test size:1000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "training, test = train_test_split(data, test_size =1000)\n",
    "print(\"Training size: {}; Test size:{}\".format(len(training), len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Determine independent features and the target feature (dependent variable) in training set and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['MaritalStatus_Num', 'Gender_Num', 'YearlyIncome', 'TotalChildren',\n",
    "       'NumberChildrenAtHome', 'EnglishEducation_Num', 'EnglishOccupation_Num',\n",
    "       'HouseOwnerFlag', 'NumberCarsOwned', 'Age']\n",
    "x_train = training[features]\n",
    "y_train = training[\"BikeBuyer\"]\n",
    "x_test = test[features]\n",
    "y_test = test[\"BikeBuyer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multi-layer perceptron in Sklearn</b>\n",
    "\n",
    "<i>https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html</i>\n",
    "\n",
    "Class MLPClassifier implements a multi-layer perceptron (MLP) algorithm that trains using Back propagation.\n",
    "\n",
    "<b><i>Syntax:</i></b>\n",
    "\n",
    "<i>class sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(100, ), activation=’relu’, solver=’adam’, alpha=0.0001, batch_size=’auto’, learning_rate=’constant’, learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08, n_iter_no_change=10)</i>\n",
    "\n",
    "<b><i>Parameters:</i></b>\n",
    "\n",
    "<br>\n",
    "<i>hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)</i>\n",
    "\n",
    "The ith element represents the number of neurons in the ith hidden layer.\n",
    "\n",
    "<br>\n",
    "<i>activation : {‘identity’, ‘logistic’, ‘tanh’, ‘relu’}, default ‘relu’</i>\n",
    "\n",
    "Activation function for the hidden layer.\n",
    "\n",
    "‘identity’, no-op activation, useful to implement linear bottleneck, returns f(x) = x\n",
    "‘logistic’, the logistic sigmoid function, returns f(x) = 1 / (1 + exp(-x)).\n",
    "‘tanh’, the hyperbolic tan function, returns f(x) = tanh(x).\n",
    "‘relu’, the rectified linear unit function, returns f(x) = max(0, x)\n",
    "\n",
    "<br>\n",
    "<i>solver : {‘lbfgs’, ‘sgd’, ‘adam’}, default ‘adam’</i>\n",
    "\n",
    "The solver for weight optimization.\n",
    "\n",
    "‘lbfgs’ is an optimizer in the family of quasi-Newton methods.\n",
    "‘sgd’ refers to stochastic gradient descent.\n",
    "‘adam’ refers to a stochastic gradient-based optimizer proposed by Kingma, Diederik, and Jimmy Ba\n",
    "\n",
    "<br>\n",
    "<i>batch_size : int, optional, default ‘auto’</i>\n",
    "\n",
    "Size of minibatches for stochastic optimizers. If the solver is ‘lbfgs’, the classifier will not use minibatch. When set to “auto”, batch_size=min(200, n_samples)\n",
    "\n",
    "<br>\n",
    "<i>learning_rate : {‘constant’, ‘invscaling’, ‘adaptive’}, default ‘constant’</i>\n",
    "Learning rate schedule for weight updates.\n",
    "\n",
    "<li>‘constant’ is a constant learning rate given by ‘learning_rate_init’.\n",
    "<li>‘invscaling’ gradually decreases the learning rate at each time step ‘t’ using an inverse scaling exponent of ‘power_t’. effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
    "<li>‘adaptive’ keeps the learning rate constant to ‘learning_rate_init’ as long as training loss keeps decreasing. Each time two consecutive epochs fail to decrease training loss by at least tol, or fail to increase validation score by at least tol <i>if ‘early_stopping’ is on, the current learning rate is divided by 5.\n",
    "Only used when solver='sgd'\n",
    "\n",
    "<br>\n",
    "<i>max_iter : int, optional, default 200</i>\n",
    "\n",
    "Maximum number of iterations. The solver iterates until convergence (determined by ‘tol’) or this number of iterations. For stochastic solvers (‘sgd’, ‘adam’), note that this determines the number of epochs (how many times each data point will be used), not the number of gradient steps.\n",
    "\n",
    "<br>\n",
    "<i>random_state : int, RandomState instance or None, optional, default None</i>\n",
    "\n",
    "If int, random_state is the seed used by the random number generator; If RandomState instance, random_state is the random number generator; If None, the random number generator is the RandomState instance used by np.random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1. Using MLP with unstandardized dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a) Define a predictive model using MLP classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(15, 2), max_iter  = 20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt = model.fit(x_train ,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Make prediction and evaluate the accuracy of the model on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.511\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(x_test)\n",
    "print(\"Accuracy:\",accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[511,   0],\n",
       "       [489,   0]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix:\")\n",
    "confusion_matrix(y_test, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Rescale/Standardize data\n",
    "\n",
    "When your data is comprised of attributes with varying scales, many machine learning algorithms can benefit from rescaling the attributes to all have the same scale.\n",
    "\n",
    "Often this is referred to as normalization and attributes are often rescaled into the range between 0 and 1. This is useful for optimization algorithms in used in the core of machine learning algorithms like gradient descent. It is also useful for algorithms that weight inputs like regression and neural networks and algorithms that use distance measures like K-Nearest Neighbors. (Jason Brownlee)\n",
    "\n",
    "Standardization of datasets is a common requirement for many machine learning estimators implemented in scikit-learn; they might behave badly if the individual features do not more or less look like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "In practice we often ignore the shape of the distribution and just transform the data to center it by removing the mean value of each feature, then scale it by dividing non-constant features by their standard deviation.\n",
    "\n",
    "The sklearn.preprocessing package provides a utility classes such as MinMaxScaler, StandardScaler that implement the Transformer API to compute the mean and standard deviation on a training set so as to be able to later reapply the same transformation on the testing set.\n",
    "\n",
    "In the below cell switch between MinMaxScaler and StandardScaler to see how well the MLP performs prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "# sc = MinMaxScaler(feature_range=(0, 1))\n",
    "sc.fit(x_train)\n",
    "x_train_std = sc.transform(x_train)\n",
    "x_test_std = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Train a model on standardized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std_model = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(15, 2), max_iter  = 20, random_state=1)\n",
    "sdt = std_model.fit(x_train_std ,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b) Make prediction and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691\n"
     ]
    }
   ],
   "source": [
    "predictions = std_model.predict(x_test_std)\n",
    "print(accuracy_score(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Comment: The MLP made slightly better prediction on standardized data than 0-1 rescaled data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Change the parameters of MLP \n",
    "\n",
    "We can increase the number of neurons in the hidden layers/ number on iteration to see if the network be able to improve the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.691\n"
     ]
    }
   ],
   "source": [
    "advanced_model = MLPClassifier(solver='lbfgs',hidden_layer_sizes=(150, 3), max_iter  = 25, random_state=1)\n",
    "sdt = advanced_model.fit(x_train_std ,y_train)\n",
    "predictions = std_model.predict(x_test_std)\n",
    "print(accuracy_score(y_test,predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
